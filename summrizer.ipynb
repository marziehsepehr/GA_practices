{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318acd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Summarizing a 500-page book using LLMs\n",
    "# ✅ Goal:\n",
    "# The book is much longer than the maximum token length of LLM models (e.g., 4096 tokens in GPT-3.5). This issue needs to be managed.\n",
    "# ✅ Solution:\n",
    "# Technique: \"Chunking + Step-by-Step Summarization\"\n",
    "\n",
    "#     Divide the book into smaller chunks (e.g., 1000 words each).\n",
    "\n",
    "#     Summarize each chunk separately.\n",
    "\n",
    "#     Combine the summaries.\n",
    "\n",
    "#     Summarize the combined summaries again (summarize the summaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4e620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static Paths: /media/marzieh/3656C28656C24679/llm_models/ t5-small\n"
     ]
    }
   ],
   "source": [
    "import config_loader\n",
    "import os\n",
    "\n",
    "# Load the configuration using the load_config function\n",
    "config = config_loader.load_config('config.py')\n",
    "\n",
    "# Access static paths or other variables from the loaded configuration\n",
    "CACHE_DIR = getattr(config, \"CACHE_DIR\", None)  # Use getattr to safely access attributes\n",
    "MODEL_NAME =getattr(config, \"MODEL_NAME\", None)\n",
    "TRANSFORMERS_OFFLINE =getattr(config, \"TRANSFORMERS_OFFLINE\", \"1\")\n",
    "if CACHE_DIR is None:\n",
    "    raise AttributeError(\"The configuration file does not contain 'CACHE_DIR'.\")\n",
    "if MODEL_NAME is None:\n",
    "    raise AttributeError(\"The configuration file does not contain 'MODEL_NAME'.\")\n",
    "print(\"Static Paths:\", CACHE_DIR,MODEL_NAME)\n",
    "\n",
    "\n",
    "# Set environment variable for offline mode\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] =TRANSFORMERS_OFFLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "084a75ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/marzieh/01D86A1423DB1480/projects/LLM/venv38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "# Load the model and tokenizer from the local directory\n",
    "print(\"Loading model from local directory...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(CACHE_DIR, MODEL_NAME))\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(CACHE_DIR, MODEL_NAME))\n",
    "\n",
    "# Initialize the summarization pipeline with the local model\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer,device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6a9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /media/marzieh/3656C28656C24679/llm_models/nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /media/marzieh/3656C28656C24679/llm_models/nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Set your custom cache directory\n",
    "NLTK_DATA_DIR = os.path.join(CACHE_DIR, \"nltk_data\")\n",
    "\n",
    "# Create nltk_data directory if it doesn't exist\n",
    "os.makedirs(NLTK_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Download the 'punkt' tokenizer to your custom nltk_data path\n",
    "nltk.download(\"punkt\", download_dir=NLTK_DATA_DIR)\n",
    "nltk.download('punkt_tab',download_dir=NLTK_DATA_DIR)\n",
    "\n",
    "# Tell NLTK to use that directory to find tokenizers and corpora\n",
    "nltk.data.path.append(NLTK_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c02cbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# Example usage (replace with your PDF processing logic)\n",
    "def summarize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Combine sentences into chunks if needed (e.g., for long texts)\n",
    "    max_chunk_length = 512  # Adjust based on model’s max input length\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_chunk_length:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    # Summarize each chunk\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=int(len(tokenizer.encode(chunk)) * 0.5), min_length=25, do_sample=False)\n",
    "        # summary = summarizer(chunk, max_length=150, min_length=30, do_sample=False)\n",
    "        summaries.append(summary[0][\"summary_text\"])\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "# Example: Read and summarize a PDF (your existing logic)\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"./The McKinsey Way Using the Techniques of the World.pdf\"  # Replace with your PDF path\n",
    "    text = read_pdf(pdf_path)\n",
    "    \n",
    "\n",
    "    summary = summarize_text(text)\n",
    "    print(\"Summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
