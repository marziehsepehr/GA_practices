{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318acd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing Large Books with LLMs\n",
    "\n",
    "# ✅ Goal:\n",
    "# Efficiently summarize a long book that exceeds the token limit of LLMs (e.g., 4096 tokens for GPT-3.5).\n",
    "\n",
    "# ✅ Approach:\n",
    "# Use a \"Chunking and Hierarchical Summarization\" strategy:\n",
    "\n",
    "#   1. Split the book into smaller text chunks (e.g., by sentences, up to a token/word limit).\n",
    "#   2. Summarize each chunk individually using the LLM.\n",
    "#   3. Concatenate all chunk summaries.\n",
    "#   4. Optionally, summarize the combined summaries for a concise final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b94cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import PyPDF2\n",
    "import config_loader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# === Load Config ===\n",
    "config = config_loader.load_config('config.py')\n",
    "CACHE_DIR = getattr(config, \"CACHE_DIR\", None)\n",
    "MODEL_NAME = getattr(config, \"MODEL_NAME\", None)\n",
    "TRANSFORMERS_OFFLINE = getattr(config, \"TRANSFORMERS_OFFLINE\", \"1\")\n",
    "\n",
    "if CACHE_DIR is None:\n",
    "    raise AttributeError(\"The configuration file does not contain 'CACHE_DIR'.\")\n",
    "if MODEL_NAME is None:\n",
    "    raise AttributeError(\"The configuration file does not contain 'MODEL_NAME'.\")\n",
    "print(\"Static Paths:\", CACHE_DIR, MODEL_NAME)\n",
    "\n",
    "# === Set Offline Mode ===\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = TRANSFORMERS_OFFLINE\n",
    "\n",
    "# === Load Model ===\n",
    "print(\"Loading model from local directory...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(CACHE_DIR, MODEL_NAME))\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(CACHE_DIR, MODEL_NAME))\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === Setup NLTK ===\n",
    "NLTK_DATA_DIR = os.path.join(CACHE_DIR, \"nltk_data\")\n",
    "os.makedirs(NLTK_DATA_DIR, exist_ok=True)\n",
    "nltk.download(\"punkt\", download_dir=NLTK_DATA_DIR)\n",
    "nltk.download('punkt_tab',download_dir=NLTK_DATA_DIR)\n",
    "nltk.data.path.append(NLTK_DATA_DIR)\n",
    "\n",
    "# === PDF Reader ===\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text\n",
    "\n",
    "# === Summarization Function ===\n",
    "def summarize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    max_chunk_length = 512  # You can tweak this\n",
    "\n",
    "    # Split text into chunks\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_chunk_length:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    # Summarize each chunk\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        input_length = len(tokenizer.encode(chunk, truncation=True))\n",
    "        max_len = min(int(input_length * 0.5), 120)\n",
    "        min_len = 25\n",
    "\n",
    "        # Fix max/min mismatch\n",
    "        if max_len <= min_len:\n",
    "            min_len = max(5, int(max_len * 0.5))\n",
    "\n",
    "        try:\n",
    "            print(f\"[INFO] Summarizing chunk with max_len={max_len}, min_len={min_len}\")\n",
    "            summary = summarizer(\n",
    "                chunk,\n",
    "                max_length=max_len,\n",
    "                min_length=min_len,\n",
    "                do_sample=False\n",
    "            )\n",
    "            summaries.append(summary[0][\"summary_text\"])\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[WARNING] Skipped chunk due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "# === Main ===\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"./The McKinsey Way Using the Techniques of the World.pdf\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "    text = read_pdf(pdf_path)\n",
    "    summary = summarize_text(text)\n",
    "    print(\"Summary:\\n\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
